{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a RAG pipeline with Anyscale, Llamaindex, and HuggingFace\n",
    "Tap into LLMs and query your data. Here's a step-by-step guide to crafting a RAG pipeline with Anyscale, Llamaindex, and HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this RAG pipeline you'll be using the following components:\n",
    "- **LLM**: [Anyscale's Llama 2 70B model](https://docs.endpoints.anyscale.com/supported-models/meta-llama-Llama-2-70b-chat-hf/) through their inference endpoint.\n",
    "- **Vectorizer**: [`bge-small-en-v1.5` embeddings model](https://huggingface.co/BAAI/bge-small-en-v1.5) from HuggingFace. You can choose other model form the [embeddings leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
    "- **Vector Store**: Llamaindex's in-memory vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.llms import Anyscale\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index import ServiceContext, download_loader, VectorStoreIndex, Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial requires an Anyscale API key. If you don't have an Anyscale API key, you can obtain one with 1 million free tokens by [registering to Anyscale](https://app.endpoints.anyscale.com/welcome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYSCALE_API_KEY = os.environ[\"ANYSCALE_API_KEY\"] # Your Anyscale API key (esecret_...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "In this section, you'll load Facebook's [Long-Context LLama 2 paper](https://ai.meta.com/research/publications/effective-long-context-scaling-of-foundation-models/) into a single LLamaindex document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA2_LONG_URL = \"https://scontent.fpei3-1.fna.fbcdn.net/v/t39.2365-6/382490704_260884199667762_4629529713553101244_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Bkh2eqWw__wAX9NLirW&_nc_ht=scontent.fpei3-1.fna&oh=00_AfA8SUU5fGbJy7ZkRsMcEmv57Y9I5kG2PzZNkBnWaCUQlg&oe=651B4045\"\n",
    "RemoteReader = download_loader(\"RemoteReader\")\n",
    "original_docs = RemoteReader().load_data(url=LLAMA2_LONG_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the loder correctly parsed the PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective Long-Context Scaling of Foundation Models\n",
      "Wenhan Xiong†∗, Jingyu Liu†, Igor Molybog,\n",
      "Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,\n",
      "Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang,\n",
      "Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan,\n",
      "Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang∗, Hao Ma∗\n",
      "Meta\n",
      "Abstract\n",
      "We present a series of long-context LLMs that support effective context windows\n",
      "of up to 32,768 tokens. Our model series are built th\n"
     ]
    }
   ],
   "source": [
    "print(original_docs[0].text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a single document from the parsed PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_content = \"\\n\\n\".join(doc.get_content() for doc in original_docs)\n",
    "docs = [Document(text=docs_content)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query engine\n",
    "Now that you've created a document, you can set up a **query engine** to query the document.\n",
    "\n",
    "In this section you'll set up a query engine using [HuggingFace embeddings](https://gpt-index.readthedocs.io/en/stable/examples/embeddings/huggingface.html) and Anyscale's LLama 2 70B endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anyscale's Llama 2 70B model, feel free to use 13B and 7B models as well\n",
    "llm = Anyscale(\"meta-llama/Llama-2-70b-chat-hf\", api_key=ANYSCALE_API_KEY)\n",
    "\n",
    "# Top-ranking lightweight embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "vector_index = VectorStoreIndex.from_documents(docs, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What's the maximum context window of LLama 2 long context?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the provided context information, the maximum context window of LLama 2 long context is 32,768 tokens. This is mentioned in the passage as the longest sequence length used in the continual pretraining process.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
